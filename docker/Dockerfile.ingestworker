
# Make it consistent with container host OS and CUDA version for better compatibility.
ARG CUDA_VERSION=13.0.0
ARG OS_VERSION=ubuntu24.04

# ==============================================================================
# BUILD STAGE
# ==============================================================================
# Use a development image with the full CUDA toolkit for compiling dependencies.
FROM nvidia/cuda:${CUDA_VERSION}-cudnn-devel-${OS_VERSION} AS builder

ARG HOME="/etc/production-demo"
ARG TIMM_MODELS=""
ARG OPENCLIP_MODEL="ViT-SO400M-16-SigLIP2-384"

# Set environment variables to prevent interactive prompts during installation.
ENV DEBIAN_FRONTEND=noninteractive \
    PYTHON_VERSION=3.13

# Centralize all model/cache downloads in an image-owned path so we can copy it
# into the runtime stage (and avoid re-downloading on every container start).
ENV XDG_CACHE_HOME="${HOME}/.cache" \
    TORCH_HOME="${HOME}/.cache/torch" \
    HF_HOME="${HOME}/.cache/huggingface" \
    HF_HUB_CACHE="${HOME}/.cache/huggingface/hub" \
    TRANSFORMERS_CACHE="${HOME}/.cache/huggingface/transformers"

RUN apt-get update &&\
    apt-get install -y --no-install-recommends \
        build-essential \
        curl \
        git 

WORKDIR "${HOME}"
# Copy dependency manifests first for better layer caching
COPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv
COPY pyproject.toml uv.lock ./
COPY ingestworker/pyproject.toml ingestworker/pyproject.toml
COPY shared shared

# Sync dependencies into venv using uv (mounted from official container)
ARG NAME
RUN --mount=type=cache,target=/root/.cache/uv,id=uv-${NAME} \
    uv sync --project ingestworker --extra gpu \
        --link-mode copy \
        --compile-bytecode  \
        --no-install-project \
        --no-editable

# Pre-download model weights into the image cache during build.
# - `timm` weights land in `$TORCH_HOME` and/or `$HF_HOME` depending on the model.
# - `open_clip` weights typically use `$HF_HOME`.
RUN mkdir -p "${XDG_CACHE_HOME}" "${TORCH_HOME}" "${HF_HOME}" && \
        if [ -n "${TIMM_MODELS}" ]; then \
            "${HOME}/.venv/bin/python" -c "import os, timm; models=os.environ.get('TIMM_MODELS','').split(); [timm.create_model(m, pretrained=True, num_classes=0, global_pool='avg') for m in models]"; \
        fi && \
        if [ -n "${OPENCLIP_MODEL}" ]; then \
            "${HOME}/.venv/bin/python" -c "import os, open_clip; name=os.environ.get('OPENCLIP_MODEL'); pt=dict(open_clip.list_pretrained()).get(name); open_clip.create_model_and_transforms(name, pretrained=pt); open_clip.get_tokenizer(name)"; \
        fi
    
# ==============================================================================
# PRODUCTION STAGE
# ==============================================================================
# Use a slim runtime image which is much smaller and more secure.
FROM nvidia/cuda:${CUDA_VERSION}-cudnn-runtime-${OS_VERSION}

ARG HOME="/etc/production-demo"

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PATH="${HOME}/.venv/bin:$PATH"
ENV PYTHONPATH="${HOME}"
ENV XDG_CACHE_HOME="${HOME}/.cache" \
    TORCH_HOME="${HOME}/.cache/torch" \
    HF_HOME="${HOME}/.cache/huggingface" \
    HF_HUB_CACHE="${HOME}/.cache/huggingface/hub" \
    TRANSFORMERS_CACHE="${HOME}/.cache/huggingface/transformers"

RUN apt-get update &&\
    apt-get install -y --no-install-recommends \
        cusparselt-cuda-13 \
        libgl1 \
        libmagic1 \
        libglib2.0-0 &&\
    rm -rf /var/lib/apt/lists/*

WORKDIR "${HOME}"

# Bring the Python install and virtual environment from the builder stage
COPY --from=builder ${HOME}/.local/share/uv/python ${HOME}/.local/share/uv/python
COPY --from=builder ${HOME}/.venv ${HOME}/.venv
COPY --from=builder ${HOME}/.cache ${HOME}/.cache
COPY conf/ingestworker.yaml conf/ingestworker.yaml
COPY ingestworker ingestworker

# Generate version.py
ARG VER
ARG NAME
ARG BUILD_TIME
ARG GIT_REVISION
RUN echo "MODULE_NAME = \"${NAME}\"\nVERSION = \"${VER}\"\nBUILD_TIME = \"${BUILD_TIME}\"\nGIT_REVISION = \"${GIT_REVISION}\"" > "${NAME}/version.py"

# ENTRYPOINT ["sleep", "infinity"]
ENTRYPOINT ["python3", "-m", "celery", "-A", "ingestworker.main:app", "worker", "--loglevel=info", "--concurrency=1", "--pool=prefork",  "-n", "ingestworker-worker@%h"]